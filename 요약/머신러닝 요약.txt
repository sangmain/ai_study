######### SVC, LinearSVC, KNeighborsClassifier, KNeighborsRegressor
from sklearn.svm import SVC, LinearSVC, KNeighborsClassifier, KNeighborsRegressor
clf = SVC()
clf.fit(x_train, y_train)
from sklearn.metrics import accuracy_score
print("정답률: ", accuracy_score(y_test, y_pred))

######### Matplotlib
#품질 데이터별로 그룹을 나누고 수 세어보기
count_data = wine.groupby('quality')["quality"].count()
print(type(count_data))

######### 수를 그래프로 그리기
count_data.plot()
plt.savefig("wine-count-plt.png")
plt.show()

######### RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=50, criterion='entropy', class_weight='balanced_subsample', warm_start=True, oob_score=True, min_samples_leaf=1, random_state=50)
model.fit(x_train, y_train)
aaa = model.score(x_test, y_test)

######### Ridge Lasso
from sklearn.linear_model import LinearRegression, Ridge, Lasso
# lasso = Lasso().fit(x, y)
# ridge = Ridge()
ridge = Ridge(alpha=0.05, normalize=True)
ridge.fit(x_train, y_train)
print(ridge.score(x_test, y_test))

######### GridSearchCV
from sklearn.model_selection import GridSearchCV
kfold_cv = KFold(n_splits=5, shuffle=True)
clf = GridSearchCV(KNeighborsClassifier(), parameters, cv = kfold_cv)


######### Pipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, StandardScaler

pipe = Pipeline([("scaler", MinMaxScaler()), ('svm', SVC())])
# from sklearn.pipeline import make_pipeline
# pipe = make_pipeline(MinMaxScaler(), SVC(C=100))

######### feature importance
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(random_state=0)
print("특성 중요도: \n", tree.feature_importances_)

def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), cancer.feature_names)

    plt.xlabel("특성 중요도")

    plt.ylabel("특성")

    plt.ylim(-1, n_features)

plot_feature_importances_cancer(tree)
plt.show()

######### GradientBoostingClassifier
from sklearn.ensemble import GradientBoostingClassifier

######### XGBClassifier
from xgboost import XGBClassifier

######### PCA
from sklearn.decomposition import PCA
pca = PCA(n_components=15)
pca.fit(x_scaled)

x_pca = pca.transform(x_scaled)

######### AutoEncoding
def build_network(optimizer='adam'):
    inputs = Input(shape=(32, 32, 3))

    encoded = Dense(32, activation='relu')(inputs)

    x = Dense(32, activation='relu')(encoded)
    decoded = Dense(3, activation='sigmoid')(x)

    model = Model(inputs = inputs, outputs=decoded)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model


######### np.save/load
import numpy as np

a = np.arange(10)
print(a)
np.save("aaa.npy", a)
b = np.load("aaa.npy")
print(b)

######### 이미지 크기 키우기
from keras.preprocessing.image import img_to_array, array_to_img
x_train = np.asarray([img_to_array(array_to_img(im, scale=False).resize((input_shape,input_shape))) for im in x_train])
x_test = np.asarray([img_to_array(array_to_img(im, scale=False).resize((input_shape,input_shape))) for im in x_test])

######### VGG16 (32,32)
conv_base = VGG16(weights="imagenet", include_top=False, input_shape=(input_shape,input_shape,3))
######### VGG19 (32,32)
######### ResNet50 (32,32)
######### InceptionV3 (75,75)
######### Xception(71,71)

######### 라벨인코더
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
encoder.fit(y)
y = encoder.transform(y)

y = to_categorical(y).astype(int)

y_pred = model.predict(x_test)
y_pred = np.argmax(y_pred, axis=1)
print(y_pred)
y_pred = encoder.inverse_transform(y_pred)