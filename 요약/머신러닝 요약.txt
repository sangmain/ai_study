######### SVC, LinearSVC, KNeighborsClassifier, KNeighborsRegressor
from sklearn.svm import SVC, LinearSVC, KNeighborsClassifier, KNeighborsRegressor
clf = SVC()
clf.fit(x_train, y_train)
from sklearn.metrics import accuracy_score
print("정답률: ", accuracy_score(y_test, y_pred))

######### Matplotlib
#품질 데이터별로 그룹을 나누고 수 세어보기
count_data = wine.groupby('quality')["quality"].count()
print(type(count_data))

######### 수를 그래프로 그리기
count_data.plot()
plt.savefig("wine-count-plt.png")
plt.show()

######### RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=50, criterion='entropy', class_weight='balanced_subsample', warm_start=True, oob_score=True, min_samples_leaf=1, random_state=50)
model.fit(x_train, y_train)
aaa = model.score(x_test, y_test)

######### Ridge Lasso
from sklearn.linear_model import LinearRegression, Ridge, Lasso
# lasso = Lasso().fit(x, y)
# ridge = Ridge()
ridge = Ridge(alpha=0.05, normalize=True)
ridge.fit(x_train, y_train)
print(ridge.score(x_test, y_test))

######### GridSearchCV
from sklearn.model_selection import GridSearchCV
kfold_cv = KFold(n_splits=5, shuffle=True)
clf = GridSearchCV(KNeighborsClassifier(), parameters, cv = kfold_cv)

######### GradientBoostingClassifier
from sklearn.ensemble import GradientBoostingClassifier

######### XGBClassifier
from xgboost import XGBClassifier

######### PCA
from sklearn.decomposition import PCA
pca = PCA(n_components=15)
pca.fit(x_scaled)

x_pca = pca.transform(x_scaled)

######### AutoEncoding
def build_network(optimizer='adam'):
    inputs = Input(shape=(32, 32, 3))

    encoded = Dense(32, activation='relu')(inputs)

    x = Dense(32, activation='relu')(encoded)
    decoded = Dense(3, activation='sigmoid')(x)

    model = Model(inputs = inputs, outputs=decoded)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model