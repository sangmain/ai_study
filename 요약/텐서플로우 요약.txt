######### Keras Optimizer LearningRate
from keras.optimizers import *
optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model.compile(loss='mse', optimizer= optimizer, metrics=['mse'])

######### Placeholder -- feed_dict
import tensorflow as tf

a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b

sess = tf.Session()
print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))

######### Reduce_Mean
loss = tf.reduce_mean(tf.square(hypothesis - y_train))
열 단위로 평균을 낸다


######### 기본 input/output
x = tf.placeholder("float", [None, 4])
y = tf.placeholder("float", [None, 3])
nb_classes = 3

w = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')
b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

######### 기본 구조
hypothesis = x * w + b

loss = tf.reduce_mean(tf.square(hypothesis - y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)
train = optimizer.minimize(loss)

######### 정답률
# 정확도 측정 (hypothesis > 0.5 기준)
# tf.cast() : true/false를 1.0/0.0 으로 반환
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))


######### softmax, one_hot

y_one_hot = tf.one_hot(y, nb_classes)
y_one_hot = tf.reshape(y_one_hot, [-1, nb_classes])

logits = tf.matmul(x, w) + b
hypothesis = tf.nn.softmax(logits)

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(
    logits = logits, labels=tf.stop_gradient([y_one_hot]))
    )

######### sigmoid
hypothesis = tf.nn.sigmoid(logits)

cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
    logits = logits, labels = y )
    )


######### dropout
layer2 = tf.nn.dropout(layer2, keep_prob = 0.5)


######### get_variable
w1 = tf.get_variable("W1", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())
b1 = tf.Variable(tf.random_normal([256]), name='bias1')
layer1 = tf.nn.relu(tf.matmul(x, w1) +  b1)