######### RNN
LSTM은 장기 기억을 해야할때 좋다	
연속적인 데이터를 가지고 다음을 예측해야할때

######### Relu

######### Tensorboard
######### LSTM stateful

######### Keras load model
from keras.models import load_model
model = load_model('savetest01.h5')

######### KerasClassifier
from keras.wrappers.scikit_learn import KerasClassifier
model = KerasClassifier(build_fn=build_network, verbose=1)

######### RandomizedSearchCV
from sklearn.model_selection import RandomizedSearchCV
search = RandomizedSearchCV(estimator=model, param_distributions=hyperparameters, n_iter=10, n_jobs=1, cv=3, verbose=1)
print(search.best_params_)

######### create_hyperparameter
def create_hyperparameter():
    batches =[32,64,128,256,500]
    optimizers = ['rmsprop', 'adam', 'adadelta']
    dropout = np.linspace(0.1, 0.5, 5)
    return{"batch_size": batches, "optimizer": optimizers, "keep_prob":dropout}

######### build_network
def build_network(keep_prob=0.5, optimizer='adam'):
    inputs = Input(shape=(784, ), name='input')

    x = Dense(512, activation ='relu', name = 'hidden1')(inputs)
    x = Dropout(keep_prob)(x)
    prediction = Dense(10, activation='softmax', name='output')(x)
    model = Model(inputs=inputs, outputs=prediction)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    return model


######### Matplot
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model acc')
plt.ylabel('acc')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()



######### Stateful
model.add(LSTM(128, batch_input_shape=(1,4,1), stateful=True))

for epochs_idx in range(num_epochs):
    history = model.fit(x_train, y_train, epochs = 1, batch_size=batch_size, verbose=2, shuffle= False, validation_data=(x_test, y_test), callbacks=[early])
    model.reset_states()
