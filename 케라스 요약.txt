######### RNN
LSTM은 장기 기억을 해야할때 좋다	
연속적인 데이터를 가지고 다음을 예측해야할

######### RMSE
from sklearn.metrics import mean_squared_error
def RMSE(y_test, y_predict):
    return np.sqrt(mean_squared_error(y_test, y_predict))
print("RMSE: ", RMSE(y_test, y_predict))

######### R2 구하기
from sklearn.metrics import r2_score
r2_y_predict = r2_score(y_test, y_predict)
print("R2: ", r2_y_predict)

######### Split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split( x, y, random_state = 66, test_size = 0.4)
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, random_state = 66, test_size = 0.5)

######### Concatenate
from keras.layers.merge import concatenate
merge1 = concatenate([output1, output2])


######### EarlyStopping
from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='loss', patience=500, mode='auto')

######### Scaler
from sklearn.preprocessing import StandardScaler, MinMaxScaler
scaler = StandardScaler()
scaler = MinMaxScaler()
scaler.fit(x)
x = scaler.transform(x)

######### Regularizers
from keras import regularizers
model.add(Dense(1000, input_shape = (input_shape, ), activation ='relu',
                kernel_regularizer= regularizers.l1(0.0001)))

######### Tensorboard
######### LSTM stateful

######### Keras load model
from keras.models import load_model
model = load_model('savetest01.h5')

######### Kfold (validation 만들기)
from sklearn.model_selection import KFold
kfold = KFold(n_splits=5, shuffle=True, random_state=seed)


######### KerasClassifier
from keras.wrappers.scikit_learn import KerasClassifier
model = KerasClassifier(build_fn=build_network, verbose=1)

######### RandomizedSearchCV
from sklearn.model_selection import RandomizedSearchCV
search = RandomizedSearchCV(estimator=model, param_distributions=hyperparameters, n_iter=10, n_jobs=1, cv=3, verbose=1)
print(search.best_params_)

######### create_hyperparameter
def create_hyperparameter():
    batches =[32,64,128,256,500]
    optimizers = ['rmsprop', 'adam', 'adadelta']
    dropout = np.linspace(0.1, 0.5, 5)
    return{"batch_size": batches, "optimizer": optimizers, "keep_prob":dropout}

######### build_network
def build_network(keep_prob=0.5, optimizer='adam'):
    inputs = Input(shape=(784, ), name='input')

    x = Dense(512, activation ='relu', name = 'hidden1')(inputs)
    x = Dropout(keep_prob)(x)
    prediction = Dense(10, activation='softmax', name='output')(x)
    model = Model(inputs=inputs, outputs=prediction)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    return model


######### Matplot
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model acc')
plt.ylabel('acc')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()



######### Stateful
model.add(LSTM(128, batch_input_shape=(1,4,1), stateful=True))

for epochs_idx in range(num_epochs):
    history = model.fit(x_train, y_train, epochs = 1, batch_size=batch_size, verbose=2, shuffle= False, validation_data=(x_test, y_test), callbacks=[early])
    model.reset_states()